{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np \n",
    "from rouge import Rouge\n",
    "\n",
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "df = df.iloc[:300]\n",
    "\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: -0.4224465489387512\n"
     ]
    }
   ],
   "source": [
    "# Q1: \n",
    "answer_llm = df.iloc[0].answer_llm\n",
    "answer_llm_embedding = embedding_model.encode(answer_llm)\n",
    "print(f\"Q1: {answer_llm_embedding[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: 31.67430877685547\n"
     ]
    }
   ],
   "source": [
    "# Q2\n",
    "evalutions = []\n",
    "for i, row in df.iterrows():\n",
    "    answer_embedding_orig = embedding_model.encode(row.answer_orig)\n",
    "    answer_embedding_llm = embedding_model.encode(row.answer_llm) \n",
    "    similarity = np.dot(answer_embedding_llm,answer_embedding_orig)\n",
    "    # save similarity to the evalutions list\n",
    "    evalutions.append(similarity)    \n",
    "\n",
    "print(f\"Q2: {np.percentile(evalutions, 75)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: 0.8362348824739456\n"
     ]
    }
   ],
   "source": [
    "# Q3 \n",
    "evalutions = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    answer_embedding_orig = embedding_model.encode(row.answer_orig, normalize_embeddings=True)\n",
    "    answer_embedding_llm = embedding_model.encode(row.answer_llm, normalize_embeddings=True) \n",
    "    \n",
    "    similarity = answer_embedding_llm.dot(answer_embedding_orig)\n",
    "    evalutions.append(similarity)    \n",
    "\n",
    "evalutions = pd.Series(evalutions)\n",
    "print(f\"Q3: {evalutions.quantile(0.75)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4: 0.45454544954545456\n"
     ]
    }
   ],
   "source": [
    "# Q4 \n",
    "rouge_scorer = Rouge()\n",
    "df_d = df.iloc[10]\n",
    "scores = rouge_scorer.get_scores(df_d.answer_llm, df_d.answer_orig)[0]\n",
    "rouge_1_f_score = scores['rouge-1']['f']\n",
    "print(f\"Q4: {rouge_1_f_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5: 0.35490034990035496\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_l = scores['rouge-l']['f']\n",
    "average_rouge = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "print(f\"Q5: {average_rouge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6 0.20696501983423318\n"
     ]
    }
   ],
   "source": [
    "# Q6\n",
    "rouge_scores = []\n",
    "rouge_1_scores = []\n",
    "rouge_2_scores = []\n",
    "rouge_l_scores = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    scores = rouge_scorer.get_scores(row.answer_llm, row.answer_orig)[0]\n",
    "    rouge_1 = scores['rouge-1']['f']\n",
    "    rouge_2 = scores['rouge-2']['f']\n",
    "    rouge_l = scores['rouge-l']['f']\n",
    "    rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "    # Create a dataframe from the scores\n",
    "    rouge_scores.append(rouge_avg)\n",
    "    rouge_1_scores.append(rouge_1)\n",
    "    rouge_2_scores.append(rouge_2)\n",
    "    rouge_l_scores.append(rouge_l)\n",
    "\n",
    "df['rouge_avg'] = rouge_scores\n",
    "df['rouge_1'] = rouge_1_scores\n",
    "df['rouge_2'] = rouge_2_scores\n",
    "df['rouge_l'] = rouge_l_scores    \n",
    "df.head(5).T\n",
    "print(f\"Q6 {df['rouge_2'].mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
